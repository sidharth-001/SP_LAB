{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOwZO5zi5TNP0S2qPsN2/cU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"32FEhVUrSQgV"},"source":["# **Author:**\n","* ### **Sidharth**\n","* ### *2K18/ MC/ 114*"]},{"cell_type":"markdown","metadata":{"id":"pggTnSFeX6_f"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RmpEYzJWSre0"},"source":["# **Objective:**\n","### Demonstrate Markov Chain Process. WAP to implement Markov Chain special cases\n","#### 1. To find steady state probabilities in case of ergodic Markov Chain.\n","#### 2. To find that the specific state in a Markov chain is a recurrent or transient."]},{"cell_type":"markdown","metadata":{"id":"7w7TdK2lTL2B"},"source":["# **Theory:**\n","### A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed. The state space, or set of all possible states, can be anything: letters, numbers, weather conditions, baseball scores, or stock performances."]},{"cell_type":"markdown","metadata":{"id":"f-03Y50wTnTV"},"source":["# **Code & Output:**"]},{"cell_type":"markdown","metadata":{"id":"V3GfgRxXXV7w"},"source":["#### 1. To find steady state probabilities in case of ergodic Markov Chain.\n"]},{"cell_type":"code","metadata":{"id":"Ffj7wyS0TqGM","executionInfo":{"status":"ok","timestamp":1603555715416,"user_tz":-330,"elapsed":1660,"user":{"displayName":"Sidharth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKYBc4FiXLN_LJ2C_0bA9UD1pxIW75rn2Bfx23xA=s64","userId":"16336888073881821839"}},"outputId":"d396a89c-eb17-4612-de66-170374d0df45","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import numpy as np\n","Arr = np.array([[0,0.5,0.5,0],[0.25,0,0.5,0.25],\n","                [0.6,0.4,0,0],[0.3,0.4,0.3,0]])\n","n = 4\n","print(\"Transition Probability Matrix: \")\n","print(Arr)\n","Arr = Arr.T\n","for i in range(0,n):\n","    Arr[i][i] -= 1\n","    Arr[0][i] = 1\n","B = [0]*n\n","B[0] = 1\n","z = np.linalg.solve(Arr,B)\n","print(\"\\nSteady State Probability Distribution given:\")\n","print(z)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Transition Probability Matrix: \n","[[0.   0.5  0.5  0.  ]\n"," [0.25 0.   0.5  0.25]\n"," [0.6  0.4  0.   0.  ]\n"," [0.3  0.4  0.3  0.  ]]\n","\n","Steady State Probability Distribution given:\n","[0.29353779 0.30668127 0.32311062 0.07667032]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kXmtTRedXYcZ"},"source":["#### 2. To find that the specific state in a Markov chain is a recurrent or transient."]},{"cell_type":"code","metadata":{"id":"8vZXcc-lUnGx","executionInfo":{"status":"ok","timestamp":1603555911613,"user_tz":-330,"elapsed":2323,"user":{"displayName":"Sidharth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKYBc4FiXLN_LJ2C_0bA9UD1pxIW75rn2Bfx23xA=s64","userId":"16336888073881821839"}},"outputId":"b79ec772-cfa0-4756-bf8d-b43e669e352e","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["import numpy as np\n","m = np.array([[1, 0 , 0 , 0, 0], [0 , 1, 0 ,0, 0] , [0 ,0 ,0.5, 0.5 ,0],\n","              [0 , 0.5, 0, 0.5, 0] , [0.25, 0.25, 0 , 0, 0.5]])\n","print(\"Transition Probability Matrix: \")\n","print(m)\n","transient=[]\n","recurrent=[]\n","row, col = m.shape\n","for i in range(row):\n","    flag=True\n","    for j in range(col):\n","        if m[i][j]>0:\n","            if m[j][i]==0:\n","                flag=False\n","                transient.append(i)\n","                break\n","    if flag :\n","        recurrent.append(i)\n","print(\"\\nTransient State:\", transient)\n","print(\"Recurrent State:\", recurrent)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Transition Probability Matrix: \n","[[1.   0.   0.   0.   0.  ]\n"," [0.   1.   0.   0.   0.  ]\n"," [0.   0.   0.5  0.5  0.  ]\n"," [0.   0.5  0.   0.5  0.  ]\n"," [0.25 0.25 0.   0.   0.5 ]]\n","\n","Transient State: [2, 3, 4]\n","Recurrent State: [0, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D09ilgGoYQhf"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lLL4tLt8VuBA"},"source":["# **Result:**\n","### Implemented Markov Chain special cases to find steady state probabilities in case of ergodic Markov Chain and to find that the specific state in a Markov chain is a recurrent or transient."]},{"cell_type":"markdown","metadata":{"id":"taZPCMz8WBdD"},"source":["# **Discussion:**\n","### The probability of transitioning to any particular state is dependent solely on the current state and time elapsed. The state space, or set of all possible states, can be anything: letters, numbers, weather conditions, baseball scores, or stock performances."]}]}